# å¯¹äºéƒ¨åˆ†æ•°æ®çš„åˆ‡åˆ†å¤„ç†
from pathlib import Path
from openai import OpenAI
import pandas as pd
from tqdm import tqdm
import json


def list_files_in_directory(directory):
    file_paths = []
    for path in Path(directory).rglob('*'):
        if path.is_file():
            file_paths.append(str(path))
    return file_paths


def save_contxt_to_file(contxt, output_file_path):
    with open(output_file_path, 'w', encoding='utf-8') as f:
        f.write(contxt)


def split_text(text, max_length=500):
    parts = []
    while len(text) > max_length:
        # æ‰¾åˆ°æœ€è¿‘çš„æ¢è¡Œç¬¦
        split_index = text.rfind('\n', 0, max_length)
        if split_index == -1:
            # å¦‚æœæ²¡æœ‰æ‰¾åˆ°æ¢è¡Œç¬¦ï¼Œåˆ™åœ¨ max_length å¤„åˆ‡åˆ†
            split_index = max_length
        parts.append(text[:split_index])
        text = text[split_index:]
    parts.append(text)
    return parts


# æ•°æ®æºå¤„ç† / æ”¶é›†éç»“æ„åŒ–æ•°æ®
def create_markdown_dataset():
    # æŒ‡å®šç›®å½•è·¯å¾„
    directory_path = 'path'

    # è·å–æ–‡ä»¶è·¯å¾„åˆ—è¡¨
    file_paths = list_files_in_directory(directory_path)

    i = 0
    # æ‰“å°æ–‡ä»¶è·¯å¾„åˆ—è¡¨
    for file_path in file_paths:
        print(file_path)
        if i == 6000:
            break

        num = str(i).zfill(4)
        try:
            with open(file_path, 'r', encoding='GBK', errors='ignore') as f:
                contxt = f.read()
        except Exception as e:
            print(f"An error occurred: {e}")
            pass  # å‘ç”Ÿå¼‚å¸¸æ—¶å¿½ç•¥å½“å‰æ–‡ä»¶å¹¶ç»§ç»­å¤„ç†ä¸‹ä¸€ä¸ªæ–‡ä»¶

        if len(contxt) < 500:
            output_file_path = f"path"
            save_contxt_to_file(contxt.replace('\n', '').replace('-', ''), output_file_path)
            i = i + 1
        else:
            parts = split_text(contxt)
            for part in parts:
                output_file_path = f"path"
                save_contxt_to_file(part.replace('\n', '').replace('-', ''), output_file_path)

                # è¡¥å……è®¡æ•°
                i = i + 1
                num = str(i).zfill(4)
                print(num)


# è®­ç»ƒæ•°æ®å¤„ç†
def create_ans_dataset():
    DATA = []

    # æŒ‡å®šç›®å½•è·¯å¾„
    directory_path = 'path'

    # openai
    client = OpenAI(
        api_key='sk',
        base_url=''
    )

    # ä¿¡æ¯æå–è§„åˆ™
    system_prompt = """1. æ³¨æ„è¾“å‡ºä»£ç æ¡†ï¼Œæ–¹ä¾¿å¤åˆ¶æå–
    2. æŒ‰ç…§è¦æ±‚å¡«è¡¥å†…å®¹å®‰å…¨ç®€æŠ¥ï¼Œå¹¶ä¸”ä¿ç•™âŒˆâŒ‹ã€[]ç¬¦å·æ–¹ä¾¿ä¿¡æ¯æå–
    3. ç”Ÿæˆ markdown ä»£ç 
    4. æ‰€æœ‰çš„â€œæ¢è¡Œâ€éƒ½ç”¨â€œ\nâ€è¡¨ç¤º

    å›å¤æ ¼å¼:
    # ğŸ»è¯­ä¹‰å†…å®¹å®‰å…¨ç®€æŠ¥ğŸ

    + ç›®æ ‡å†…å®¹ç±»åˆ«ï¼š?
    + æ¶‰åŠå†…å®¹å±æ€§ï¼š?
    + å®‰å…¨ç­‰çº§åˆ’åˆ†ï¼š[ä½ã€ä¸­æˆ–è€…é«˜?]å±é™©ç­‰çº§
    + åˆ’åˆ†ç†ç”±ï¼š?

    ## å…·ä½“å†…å®¹åˆ†æ

    ### åˆ†æç‚¹ä¸€

    æˆªå–ç›®æ ‡å†…å®¹ï¼šâŒˆâŒ‹
    åˆ†æç»“æœï¼š?

    ### åˆ†æç‚¹äºŒ

    æˆªå–ç›®æ ‡å†…å®¹ï¼šâŒˆâŒ‹
    åˆ†æç»“æœï¼š?

    ### åˆ†æç‚¹ä¸‰

    æˆªå–ç›®æ ‡å†…å®¹ï¼šâŒˆâŒ‹
    åˆ†æç»“æœï¼š?

    ## ç»“è®ºæ€»ç»“
    ?
    """

    # è·å–æ–‡ä»¶è·¯å¾„åˆ—è¡¨
    file_paths = list_files_in_directory(directory_path)

    i = 0
    # æ‰“å°æ–‡ä»¶è·¯å¾„åˆ—è¡¨
    for file_path in tqdm(file_paths):
        if i <= -1:
            i = i + 1
            continue

        with open(file_path, 'r', encoding='utf-8') as f:
            content = f.read()

        CONTEXT1 = f"""{content}"""

        query_prompt = f"""è¯·åˆ†æä»¥ä¸‹æ–‡æœ¬:\n{CONTEXT1}"""
        # deepseek
        completion = client.chat.completions.create(
            model="deepseek-chat",
            messages=[
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": query_prompt}
            ],
            top_p=0.7,
            temperature=0.9
        )

        CONTEXT2 = completion.choices[0].message.content

        _data = {
            'messages': [
                {'role': 'system', 'content': 'ä½ æ˜¯ AI ä¿¡æ¯å†…å®¹å®‰å…¨ä¸“å®¶'},
                {'role': 'user', 'content': f'è¯·ä½ åˆ†æä»¥ä¸‹å†…å®¹:\n```\n{CONTEXT1}\n```\nè¾“å‡ºåˆ†æç»“æœ'},
                {'role': 'assistant', 'content': f'åˆ†æç»“æœå¦‚ä¸‹:\n---\n{CONTEXT2}\n---\n'}
            ]
        }

        num = str(i).zfill(4)
        DATA.append(_data)
        with open(f'path/train_full_{num}.json', 'w', encoding='utf-8') as f:
            json.dump(_data, f, ensure_ascii=False, indent=4)

        i = i + 1

    with open(f'path', 'w', encoding='utf-8') as f:
        json.dump(DATA, f, ensure_ascii=False, indent=4)


def synthesis_compo_dataset():
    # open and list the folder
    directory = "path"
    file_paths = list_files_in_directory(directory=directory)

    # setting
    DATA = []
    cnt_1 = 0  # ```markdown
    cnt_2 = 0  # ```
    cnt_3 = 0  # ä¿®æ”¹å -> ```markdown
    cnt_4 = 0  # ä¿®æ”¹å -> ```
    for file_path in file_paths:
        if "```markdown":
            cnt_1 += 1

        with open(file_path, 'r', encoding="utf-8") as f:
            contxt = json.load(f)
            contxt["messages"][2]["content"] = contxt["messages"][2]["content"].replace("```markdown", "")
            if "```":
                cnt_2 += 1
            contxt["messages"][2]["content"] = contxt["messages"][2]["content"].replace("```", "")
            if not "```markdown":
                cnt_3 += 1
            if not "```markdown":
                cnt_4 += 1
            DATA.append(contxt)

    print("cnt_1: ", cnt_1)
    print("cnt_2: ", cnt_2)
    print("cnt_3: ", cnt_3)
    print("cnt_4: ", cnt_4)
    with open(f'path', 'w', encoding='utf-8') as f:
        json.dump(DATA, f, ensure_ascii=False, indent=4)


if __name__ == "__main__":
    pass
    # create_markdown_dataset()
    # create_ans_dataset()
    # synthesis_compo_dataset()
